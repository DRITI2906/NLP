{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fed91d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29b24248-3bdb-4508-ba84-5ed60c7941fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(path=\"ai4bharat/IndicCorpV2\", split= \"hin_Deva\", streaming=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85a46568-0db3-4888-b6dd-da92c36ae126",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = load_dataset(path=\"ai4bharat/IndicCorpV2\", split= \"guj_Gujr\", streaming=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8132196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60279768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "{'text': 'આ વીડિયો જુઓ: ઊંઝા માર્કેટયાર્ડ આજથી 25 જુલાઈ સુધી બંધ'}\n",
      "------\n",
      "Example 2:\n",
      "{'text': ''}\n",
      "------\n",
      "Example 3:\n",
      "{'text': 'મિથેનોલ આવ્યો ક્યાંથી?'}\n",
      "------\n",
      "Example 4:\n",
      "{'text': ''}\n",
      "------\n",
      "Example 5:\n",
      "{'text': 'આખરે ત્રણ રાજ્યોમાં મળેલ હાર પર કોંગ્રેસ અધ્યક્ષ રાહુલ ગાંધી દ્વારા પ્રથમ પ્રતિક્રિયા આપવામાં આવી છે. તેમણે કહ્યું કે, ત્રિપુરા, નાગાલેન્ડ અને મેઘાલયમાં લોકોના જનાદેશનો સ્વાગત કરીએ છે અને આ ક્ષેત્રના લોકોનો વિશ્વાસ ફરીથી જીતીવા માટે પ્રતિબદ્ધ છીએ.'}\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "for i, example in enumerate(islice(ds2, 5)):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(example)\n",
    "    print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a461aa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'આ વીડિયો જુઓ: ઊંઝા માર્કેટયાર્ડ આજથી 25 જુલાઈ સુધી બંધ'}\n",
      "{'text': ''}\n",
      "{'text': 'મિથેનોલ આવ્યો ક્યાંથી?'}\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(ds2):\n",
    "    print(sample)\n",
    "    if i == 2:  \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc26ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#para to sentence tokenizer \n",
    "\n",
    "import re\n",
    "\n",
    "def gujarati_sentence_tokenizer(text):\n",
    "    # Replace multiple newlines or whitespace with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Add a marker to split on Gujarati and English sentence-ending punctuations\n",
    "    sentence_endings = r'[।!?]|(?<!\\d)\\.(?!\\d)'  # Avoid splitting on decimal points like 3.14\n",
    "    sentences = re.split(sentence_endings, text)\n",
    "    \n",
    "    # Strip leading/trailing whitespace and remove empty sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9a546fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: મારું નામ રવિ છે\n",
      "Sentence 2: હું અમદાવાદમાં રહું છું\n",
      "Sentence 3: શું તું મને ઓળખે છે\n",
      "Sentence 4: હું બહુ ખુશ છું\n"
     ]
    }
   ],
   "source": [
    "gujarati_paragraph = \"મારું નામ રવિ છે. હું અમદાવાદમાં રહું છું! શું તું મને ઓળખે છે? હું બહુ ખુશ છું।\"\n",
    "sentences = gujarati_sentence_tokenizer(gujarati_paragraph)\n",
    "\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"Sentence {i}: {sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b1b7c8f-bec1-4720-922c-9c9d27711dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example text: આ વીડિયો જુઓ: ઊંઝા માર્કેટયાર્ડ આજથી 25 જુલાઈ સુધી બંધ\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for i, sample in enumerate(ds2):\n",
    "    if \"text\" in sample:\n",
    "        texts.append(sample[\"text\"])\n",
    "    if i >= 999:\n",
    "        break\n",
    "\n",
    "print(\"Example text:\", texts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c00177-6ae0-411d-9919-bf08a7a2820e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['હું', 'જઇ', 'રહ્યો', 'છું', 'અને', 'દૂધ', '!']\n"
     ]
    }
   ],
   "source": [
    "#sentence to words \n",
    "\n",
    "def gujarati_word_tokenizer(text):\n",
    "    text = re.sub(r'[“”\"\\'‘’]', '', text)   #  quotes\n",
    "    text = re.sub(r'[\\n\\r\\t]', ' ', text)   #  newlines/tabs with space\n",
    "    text = re.sub(r'[।|]', '.', text)       # danda or pipes to period\n",
    "    text = re.sub(r'\\s+', ' ', text)        # collapse multiple spaces\n",
    "    text = re.sub(r'([.,!?()])', r' \\1 ', text)  # isolate punctuation\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)        # clean again\n",
    "\n",
    "    #Extracting guj words\n",
    "    gujarati_words = re.findall(r'[\\u0A80-\\u0AFF]+', text)\n",
    "    \n",
    "    #  Extracting punctuation\n",
    "    punctuation_tokens = re.findall(r'[.,!?()]', text)\n",
    "\n",
    "    all_tokens = re.findall(r'[\\u0A80-\\u0AFF]+|[.,!?()]', text)\n",
    "\n",
    "    return all_tokens\n",
    "\n",
    "sentence = \"હું market જઇ રહ્યો છું because I want to buy fruits અને દૂધ!\"\n",
    "tokens = gujarati_word_tokenizer(sentence)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78536ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         r'\\d+\\.\\d+|' #floating point\n",
    "#         r'\\d+|' #integer\n",
    "#         r'[\\w\\.-]+@[\\w\\.-]+|' #email-address\n",
    "#         r'\\w+://\\S+|' #URL\n",
    "#         r'[^\\s\\w]', #punctuation and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9283115b-42f8-4214-8007-08a1093631ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentences saved to tokenized_output.txt\n"
     ]
    }
   ],
   "source": [
    "output_file = \"tokenized_output.txt\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, example in enumerate(ds2):\n",
    "        paragraph = example[\"text\"]\n",
    "        \n",
    "        sentences = gujarati_sentence_tokenizer(paragraph)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            tokens = gujarati_word_tokenizer(sentence)\n",
    "            if tokens:\n",
    "                f.write(\" \".join(tokens) + \"\\n\")\n",
    "\n",
    "        if i == 999:\n",
    "            break\n",
    "\n",
    "print(f\"Tokenized sentences saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9bfb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Output saved to tokenized.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "all_sentences = []\n",
    "all_tokens = []\n",
    "\n",
    "for i, example in enumerate(ds2):\n",
    "    paragraph = example[\"text\"]\n",
    "    \n",
    "    sentences = gujarati_sentence_tokenizer(paragraph)\n",
    "    all_sentences.extend(sentences)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = gujarati_word_tokenizer(sentence)\n",
    "        clean_tokens = [tok for tok in tokens if tok.strip() not in ['', '।', '|', '.', ',']]  # filter noise\n",
    "        if clean_tokens:\n",
    "            all_tokens.append(clean_tokens)\n",
    "\n",
    "    if i == 999:\n",
    "        break\n",
    "\n",
    "output_data = {\n",
    "    \"sentences\": all_sentences,\n",
    "    \"tokens\": all_tokens\n",
    "}\n",
    "\n",
    "output_file = \"tokenized.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Output saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774f536d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences: 1548\n",
      "Total Words: 21856\n",
      "Total Characters: 106508\n",
      "Average Sentence Length: 14.12\n",
      "Average Word Length: 4.87\n",
      "Type/Token Ratio (TTR): 0.3781\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"tokenized.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "tokens_per_sentence = data.get(\"tokens\", [])\n",
    "\n",
    "total_sentences = len(tokens_per_sentence)\n",
    "\n",
    "words = [word for sentence in tokens_per_sentence for word in sentence]\n",
    "total_words = len(words)\n",
    "\n",
    "total_characters = sum(len(word) for word in words)\n",
    "\n",
    "avg_sentence_length = total_words / total_sentences if total_sentences > 0 else 0\n",
    "\n",
    "avg_word_length = total_characters / total_words if total_words > 0 else 0\n",
    "\n",
    "unique_tokens = set(words)\n",
    "ttr = len(unique_tokens) / total_words if total_words > 0 else 0\n",
    "\n",
    "print(f\"Total Sentences: {total_sentences}\")\n",
    "print(f\"Total Words: {total_words}\")\n",
    "print(f\"Total Characters: {total_characters}\")\n",
    "print(f\"Average Sentence Length: {avg_sentence_length:.2f}\")\n",
    "print(f\"Average Word Length: {avg_word_length:.2f}\")\n",
    "print(f\"Type/Token Ratio (TTR): {ttr:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
